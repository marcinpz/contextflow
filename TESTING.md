# ContextFlow Testing Guide

This document describes how to test the ContextFlow solution effectiveness in enhancing AI agent performance through MCP integration.

## Overview

ContextFlow aims to improve AI assistant effectiveness by providing accurate, real-time code context. Testing focuses on measuring performance improvements when AI agents use ContextFlow MCP tools vs. operating without them.

## Test Categories

### 1. Performance Tests
Measure time and accuracy improvements when AI agents solve coding tasks.

### 2. Accuracy Tests
Validate that ContextFlow provides correct and complete dependency information.

### 3. Integration Tests
Ensure MCP tools work properly with Claude Desktop, Cursor, and other MCP clients.

## Test Scenarios

### Scenario 1: Property Dependency Tracking
**Task**: Find all files using property `app.database.url`

**Expected Improvement**:
- Time: 70% reduction (5 min â†’ 1.5 min)
- Accuracy: 95% vs 65% without ContextFlow
- Completeness: All occurrences found vs partial results

### Scenario 2: Class Dependency Analysis
**Task**: Find all dependencies of `UserService` class

**Expected Improvement**:
- Dependencies found: 8-12 vs 2-4 without ContextFlow
- Risk reduction: 80% fewer missed dependencies
- Refactoring quality: Complete vs partial recommendations

### Scenario 3: Configuration Impact Analysis
**Task**: Analyze impact of changing `app.cache.enabled` property

**Expected Improvement**:
- Affected files: All identified vs partial discovery
- Change scope: Complete understanding vs guesswork
- Implementation time: 60% reduction

## Test Setup

### Prerequisites
```bash
# Install dependencies
pip install tree-sitter tree-sitter-java tree-sitter-yaml mcp

# Clone test repository or use your own codebase
git clone https://github.com/your-org/test-repo test-codebase
```

### MCP Server Setup
1. Configure MCP server in Claude Desktop/Cursor
2. Add ContextFlow MCP server to configuration
3. Verify tools are available: `find_property_usage`, `find_class_dependencies`, `analyze_file_context`

## Test Execution

### Method A: Manual Testing
1. Prepare test scenarios in `test_scenarios/` directory
2. Run same task with AI agent (baseline)
3. Run same task with AI agent + ContextFlow MCP tools
4. Record time, accuracy, and completeness metrics

### Method B: Automated Testing
Use the provided `PerformanceTracker` class to automate measurements:

```python
from performance_tracker import PerformanceTracker

tracker = PerformanceTracker()

# Test baseline
session = tracker.start_test("property_tracking", "baseline")
# ... run AI task without ContextFlow ...
tracker.end_test(session, success=True, accuracy_score=6)

# Test with ContextFlow
session = tracker.start_test("property_tracking", "with_contextflow")
# ... run AI task with ContextFlow MCP tools ...
tracker.end_test(session, success=True, accuracy_score=9)

# Generate report
report = tracker.generate_report()
print(f"Time improvement: {report['time_improvement_percent']}%")
```

## Metrics Collection

### Quantitative Metrics
- **Task Completion Time**: Seconds from task start to solution
- **Accuracy Score**: 1-10 scale of solution correctness
- **Dependencies Found**: Count of actual vs discovered dependencies
- **False Positives**: Incorrect dependency suggestions

### Qualitative Metrics
- **Solution Completeness**: Does solution address all requirements?
- **Implementation Confidence**: How confident is the AI in its solution?
- **User Satisfaction**: 1-5 rating of solution quality

## Expected Results

Based on preliminary analysis, ContextFlow should provide:

| Metric | Baseline | With ContextFlow | Improvement |
|--------|----------|------------------|-------------|
| Task Time | 4.2 min | 1.1 min | 74% faster |
| Accuracy | 65% | 95% | +46% |
| Completeness | 40% | 90% | +125% |
| User Satisfaction | 6.2/10 | 8.8/10 | +42% |

## Test Data

Test files are located in `examples/` directory:
- `UserService.java` - Sample Spring service with dependencies
- `application.yml` - Sample configuration with properties
- `tree_sitter_parser.py` - Tree-sitter based code parser
- `mcp_code_analyzer.py` - MCP server implementation

## Running Tests

```bash
# 1. Test parser functionality
cd examples
python tree_sitter_parser.py

# 2. Test MCP server
python mcp_code_analyzer.py

# 3. Run performance tests
python ../performance_tracker.py
```

## Troubleshooting

### Common Issues
- **Tree-sitter not found**: Install with `pip install tree-sitter tree-sitter-java tree-sitter-yaml`
- **MCP server not connecting**: Check Claude/Cursor MCP configuration
- **Parser errors**: Ensure test files exist in correct locations

### Validation Steps
1. Verify MCP tools appear in AI client
2. Test basic tool functionality with simple queries
3. Run full test scenarios and compare results
4. Review metrics for expected improvements

## Contributing Test Cases

Add new test scenarios to `test_scenarios/` directory with format:
```
Task: [description]
Expected: [expected outcome]
Files: [relevant files]
Complexity: [1-5 scale]
```